---
title: "Larger scale acceptance sampling"
date: "2017-01-08"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Larger scale acceptance sampling}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

# Introduction

Sometimes one wishes to sample (perhaps for quality) a large number of
items to determine that the occurrence probability of unusual cases is
sufficiently low; otherwise known as acceptance sampling.
Statistically, this is the problem of hypothesis testing on a binomial
probability.  Some efficiencies can be gained by using a simple
group-sequential design, but that does require computation to
determine its operating characteristics.

This is also handled by the ```gsDesign``` R package (specifically the
```gsBinomialExact``` function therein); this package has one
essential difference to that one - it is _fast_.

# Example

Herein we suppose that we're sampling a $250000$-item database, and we
want to be sure that the occurrence probability is less than $0.1\%$.
We suppose we'll look at the database after $5000$, $10000$, $20000$,
and $100000$ items.  After $5000$ items we'll stop sampling if we've
seen $0$ cases.  After $10000$ items we'll stop if we've seen
$3$ or fewer cases.  After $20000$ items we'll stop if we've see $11$
or fewer cases.  And after $100000$ items we'll stop if we've seen
$78$ or fewer cases.

```{r}
steps <- c(5000, 10000, 20000, 100000)
crits <- c(0, 3, 11, 78)
```

We can look at the operating characteristics thereof relatively
simply.  We first have a look at the stopping probabilities when the
true event probability is $0.1\%$.

```{r}
library("dbSamplr")

tot.1 <- gsProbs(0.1/100, steps, crits)
## stopping probabilities at each stage
tot.1
## mean number of records sampled
tot.1 %*% steps + 250000 * (1 - sum(tot.1))
## total stopping probability
sum(tot.1)
```

Now let's have a look across a range of probabilities.

```{r}
test.probs <- c(0.01, 0.05, 0.075, 0.1, 0.2) / 100

stop.probs <- vapply(test.probs,
                     FUN=gsProbs, FUN.VALUE=steps,
                     steps, crits)
```

The probability of early stopping can then be plotted as a function of
the _true_ event probability.

```{r echo=FALSE}
plot(test.probs, colSums(stop.probs),
     type='l',
     xlab="True probability",
     ylab="Early stopping probability")
```

The expected sample sizes can also be plotted.

```{r echo=FALSE}
plot(test.probs,
     steps %*% stop.probs + (1 - colSums(stop.probs)) * 250000,
     type='b',
     ylab='Expected sample size',
     xlab='True probability')
```

We can also look at the probability of stopping at each stage.

```{r echo=FALSE}
matplot(steps, stop.probs, type='l',
        log="x",
        axes=FALSE, ylim=c(0, 1),
        xlab="Sample point",
        ylab="Stopping probability")
axis(1, at=steps, label=format(steps, scientific=FALSE))
axis(2, las=1)
```

# Algorithmics

The reason for the speed difference is very simple; this package very 
carefully doesn't do anything it knows it doesn't need to.

Note that the algorithms used herein are $O(N^2)$.  With some effort
it is possible to reduce this to $O(N\log N)$ using the fact that
Toeplitz matrix vector multiplications can be done using FFT.

